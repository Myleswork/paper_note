DML方法打破了高性能教师网络和小规模学生网络的“强弱关系”，提出了一种深度相互学习策略，即deep mutual learning。这一方法使一组简单的学生网络在整个训练过程中相互学习、相互指导。

<img title="" src="file:///C:/Users/mings/AppData/Roaming/marktext/images/2024-01-09-10-42-21-image.png" alt="" width="295" data-align="center">

文章中指出，在训练过程中，每个学生网络的损失函数由两部分组成：

（1）传统的监督学习损失（conventional supervised learning loss）

（2）用于和其他学生模型进行匹配的模仿损失（KLD-based mimcry loss to match the probability estimate of its peers）

这样的损失设置也是作者认为DML这种结构能够可行的原因。学生模型互相学习看似是一种“瞎子带领瞎子”的方法，但实则不然。首先，传统的监督学习损失能够让所有的学生模型的性能实现持续提升，且不会陷入群体思维，通过监督学习，所有的学生模型很快就会对每个训练实例做出逼近正确的预测，而不同的初始参数又使得每一个学生模型的结果不尽相同；而模仿损失的设置是为了学生模型之间互相学习，得到“额外的知识”。这就是DML能够打破传统知识蒸馏中“强弱关系”的原因，原先由软目标提供的“知识”，现在通过学生模型之间互学习得到。

DML这种基于相互学习的在线知识蒸馏思想，不仅解决了传统知识蒸馏思想中，训练教师模型产生的高成本问题。还带来了以下优势：

1.网络训练效率随着集合中学生网络数量增加而增加；

2.更小的模型带来更小的计算成本，能够在一块GPU上进行DML训练；

3.相互学习适用于各种不同的网络架构，以及由不同规模网络构成的异构群组；

4.较大规模的模型通过相互学习训练，相较于单独训练，也能获得更好的性能；

5.相互学习队列也能整合为一个高效的集成网络。

DML作为在线知识蒸馏的开山之作，创新性不言而喻。但同时也存在一些问题，学生模型之间的输出是各不相同的，且可能存在相互之间完全对立的结果，当模型之间性能不同时，这样的结果会成为模型获得更高性能的阻碍。可以设置一个滤波器，或者针对不同性能设置不同的权重，对所有学生模型产生的软目标进行整合，再让下一轮学生模型进行拟合。

为了支撑上述作者提出的优势，本人认为至少需要进行下列实验：

1.单独模型训练以及队列模型训练的得到的性能比较实验；

2.小规模模型的DML以及较大规模模型的DML性能比较实验；

3.相同网络架构的DML和不同网络架构的DML性能比较实验；

4.DML和传统知识蒸馏性能比较实验。

从文章中提到的实际完成的实验中，作者的实验安排还是比较合理的。主要做了两个方面的实验，利用CIFAR-100数据集和Market-1501数据集进行了一系列目标分类和任务重识别任务检测。用到的模型包括体量较小的ResNet-32(0.5M)、MobileNet(3.3M)、InceptionV1(7.8M)以及体量较大的模型WRN-28-10(36.5M)。

首先作者在CIFAR-100数据集上进行了实验，对只有两个网络的DML进行测试采用不同的网络结构，获得了一系列Top-1指标，结果如表1所示。

可以看到，相对于独立的分类网络，添加DML机制后，无论模型组合方式如何，在性能上均得到的显著的提升。相对于体量较大的模型，体量较小的模型加入DML机制后，得到的性能提升更为显著。从这一实验中可以看出，打破传统知识蒸馏思想的“强弱关系”，也能得到性能的提高。

**Table 1**  Top-1 accuracy (%) on the CIFAR-100 dataset.

**表1**  CIFAR-100数据集上得到的Top-1指标

<img src="file:///C:/Users/mings/Desktop/1.jpg" title="" alt="1.jpg" data-align="center">

此外，作者在相互学习层面也进行了实验。在Market-1501数据集上进行mAP和rank-1指标测试。每个MobileNet在单队列和多队列（队列由两个网络构成）中训练，并计算队列中两个网络的平局性能。并与当前主流的方法进行对比。结果如表3所示。

**Table 2**  Comparative results on the Market-1501 dataset

**表2**  Market-1501数据集上的比较实验结果

<img src="file:///C:/Users/mings/Desktop/2.jpg" title="" alt="2.jpg" data-align="center">

可以发现，相对于单独学习，加入DML策略的MobileNet性能得到了显著提升，且均显著优于主流方法。

由于DML是区别于传统知识蒸馏方法的全新方法，所以作者设置了两者的对比实验。实验设置了三组网络在CIFAR-100上的Top-1指标和Market-1501上的mAP指标对照实验。采用单独训练、传统蒸馏、DML三种方式，结果如表3所示。

**Table 3**  Comparison with distillation on CIFAR-100 (Top-1 accuracy (%)) and Market-1501 dataset (mAP (%))

**表3**  CIFAR-100和Market-1501数据集的蒸馏对照实验

<img title="" src="file:///C:/Users/mings/Desktop/3.jpg" alt="3.jpg" data-align="center">

从实验结果中可以看到，传统蒸馏思想下，学生模型的性能提升微乎其微，甚至有所下降。而采用DML策略进行训练时，不仅仅是学生模型的性能得到了显著提升，教师模型的性能也得到了一定的提升，尤其是在两个规模较小的模型之间采用DML策略进行训练时，提升尤其显著。也证明了DML这一在线蒸馏方法优于传统的蒸馏方法。

在上面的实验中，训练队列均由两个模型组成。所以作者设置了以队列中模型数量为变化量的对照实验。实验在Market-1501数据集上进行，采用mAP作为评价指标，结果如图2所示。

**Fig 2**  Performance (mAP (%)) on Market-1501 with different numbers of networks in cohort

**图2**  Market-1501数据集上的序列中不同模型数量的性能

<img src="file:///C:/Users/mings/Desktop/4.jpg" title="" alt="4.jpg" data-align="center">

从结果中可以发现，采用DML策略训练的网络，随着徐磊中模型数量的增加，性能随之提升。且DML策略训练后的集成网络的性能也由于单独训练的集成网络。

上述实验非常有利地支撑了论文中提出的一系列观点，与现有方法进行了充分比较，且实验数据丰富且数据量足够大。实验结果具有可重复性（并没有任何随机的因素）。

在结论部分，作者对DML进行了总结，对优势以及应用场景做了简单的概述。作为在线蒸馏的开创性论文，DML必然是非常重要的，有效地解决了传统知识蒸馏的一系列问题。

本文在故事的写作、实验的设置以及理论的说明上有比较强的逻辑性，比较清晰。尤其是对DML的作用机制做了理论和实验两方面的描述。

参考文献包括了数据集的对应文献、传统蒸馏方法的经典之作以及DML理论说明部分的相关论文。参考文献足够，反映了当时两年该领域的最新研究。
